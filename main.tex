\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{minted}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[style=ieee,backend=biber]{biblatex}
\addbibresource{references.bib}
\usepackage{appendixnumberbeamer}
\pdfstringdefDisableCommands{%
\def\translate#1{#1}%
}
\beamertemplatenavigationsymbolsempty
\usetheme{Madrid}
\usecolortheme{default}
\AtBeginSection[]
{
 \begin{frame}
 \frametitle{Table of Contents}
 \tableofcontents[currentsection, currentsubsection]
 \end{frame}
}

\title[Gradient-Based Optimization Algorithms]{Gradient-Based Optimization Algorithms}
\subtitle{CS115.Q11}
\author[Vinh, Kha]{N.~H.~Vinh\inst{1} \and N.~H.~Kha\inst{1}}
\institute[VNU-UIT]{
  \inst{1}%
  Department of Computer Science\\
  University of Information and Technology
}
\date[October 2025]{CS115 Presentation}


%% Page number at footer
\expandafter\def\expandafter\insertshorttitle\expandafter{%
 \insertshorttitle\hfill%
 \insertframenumber\,/\,\inserttotalframenumber}

\begin{document}

%% Title frame
\begin{frame}
	\titlepage
\end{frame}

%% ToC frame
\begin{frame}{Table of Contents}
	\tableofcontents
\end{frame}

% ======================
\section{Preliminaries} 
% ======================

\subsection{Notation}
\begin{frame}{Preliminaries - Notation}
    \begin{itemize}
        \item $\mathbf{x}_i$ is the $i$-th input vector from the dataset.
        \item $y_i$ is the corresponding label for $\mathbf{x}_i$.
        \item $N$ is the total number of samples.
        \item $\mathcal{D} = \{ (\mathbf{x}_i, y_i) \}_{i=1}^N$ is the dataset.
        \item $\theta$ is the parameter of the model.
        \item $\theta^*$ is the optimal solution.
        \item $\eta$ is the learning rate
        \item $g(\theta_t; \xi_t) $ is the stochastic gradient on random mini-batch $\xi_t$ ($\mathbb E[g(\theta_t; \xi_t)] = \nabla \mathcal L(\theta_t;\mathcal D)$
    \end{itemize}
\end{frame}
    
\subsection{Mathematical Formulas}
\begin{frame}[fragile,allowframebreaks]{Preliminaries - Mathematical Formulas}
    \begin{itemize}
        \item Gradient Vector:
        \[
            \nabla F(x) = 
            \left( 
                \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}
            \right)
        \]

        \item Hessian Matrix:
        \[
            \nabla^2 F(x) = 
            \begin{bmatrix}
                \frac{\partial^2 f}{\partial x_1^2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
                \vdots & \ddots & \vdots \\
                \frac{\partial^2 f}{\partial x_n \partial x_1} & \dots & \frac{\partial^2 f}{\partial x_n^2}
            \end{bmatrix}
        \]
        \item Jacobian ($f: \mathbb{R}^n \rightarrow \mathbb{R}^m$):
    \[
        J_f(x) =
        \begin{bmatrix}
            \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n}
        \end{bmatrix}
    \]
    \item Hadamard Product (element-wise product) of two vectors $\mathbf{a}, \mathbf{b} \in \mathbb{R}^n$:
    \[
        \mathbf{a} \odot \mathbf{b} = 
        \begin{bmatrix}
            a_1 b_1 \\
            a_2 b_2 \\
            \vdots \\
            a_n b_n
        \end{bmatrix}
    \]
        \item Element-wise Square (Hadamard power 2):
    \[
        \mathbf{a}^2 = \mathbf{a} \odot \mathbf{a} =
        \begin{bmatrix}
            a_1^2 \\
            a_2^2 \\
            \vdots \\
            a_n^2
        \end{bmatrix}
    \]

    \item Element-wise Square Root: 
    \[
        \sqrt{\mathbf{a}} =
        \begin{bmatrix}
            \sqrt{a_1} \\
            \sqrt{a_2} \\
            \vdots \\
            \sqrt{a_n}
        \end{bmatrix}
    \]
    
    \end{itemize}

    

\end{frame}
\subsection{Loss Gradient Function}
\begin{frame}[fragile,allowframebreaks]{Preliminaries - Loss Gradient Function}
    The gradient function:
    \[
        \nabla L(\theta) = \frac{2}{m} \mathbf x^{\top}(\mathbf x \theta + y)
    \]

    \begin{minted}[fontsize=\small, bgcolor=gray!10]{python}
def grad_func(X, y, theta):
    m = len(y)
    gradients = (2/m) * X.T.dot(X.dot(theta) - y)
    return gradients
    \end{minted}
\end{frame}

% ======================
\section{First-Order Methods} 
% ======================

\subsection{Gradient Descent}

\begin{frame}{First-Order Methods}
\framesubtitle{Gradient Descent}
\begin{block}{Gradient Descent}
Update formula:
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal L(\theta_t, \mathcal D),
\]
\end{block}
\end{frame}

\begin{frame}[fragile]{First-Order Methods}
\framesubtitle{Gradient Descent}
\small
\begin{algorithm}[H]
\caption{Gradient Descent}
\begin{algorithmic}[1]
\Require $\eta$: Learning rate
\Require $\mathcal{L}(\theta)$: Loss function
\Require $\theta_0$: Initial parameter
\State $t \gets 0$
\While {not converged} \Comment{Initialize step}
    \State $g_t \gets \nabla_\theta \mathcal{L}(\theta_t)$ \Comment{Compute gradient}
    \State $\theta_{t+1} \gets \theta_t - \eta g_t$ \Comment{Update parameter}
    \State $t \gets t + 1$ \Comment{Update step}
\EndWhile
\State \Return $\theta_{t+1}$
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{First-Order Methods}
\framesubtitle{Gradient Descent}
\begin{minted}[fontsize=\small, bgcolor=gray!10, linenos]{python}
def gradient_descent(grad_fn, theta0, eta, T, X, y):
    for t in range(T):
        g = grad_func(X, y, theta)
        theta0[:] = theta0 - eta * g
\end{minted}
\end{frame}

\begin{frame}{First-Order Methods}
\framesubtitle{Gradient Descent}
\textbf{Problems:}
\begin{itemize}
    \item Use all training data, can be really \textbf{inefficient} with huge dataset.
    \item Cannot escape \textbf{local minima, saddle points}.
    \item The \textbf{lack of generalization} ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function.
\end{itemize}
\end{frame}

\begin{frame}{First-Order Methods}
\framesubtitle{Stochastic Gradient Descent}
\begin{block}{Stochastic Gradient Descent}
Update formula:
\[
\theta_{t+1} = \theta_t - \eta g(\theta_t, \xi_t ),
\]
\end{block}
\begin{itemize}
    \item Uses only a small subset of data (mini-batch or single sample) at each update, leading to \textbf{faster computation}.
    \item Adds \textbf{stochastic noise} to the gradient, which helps the optimizer \textbf{escape local minima} and explore the loss landscape.
\end{itemize}
\end{frame}

\begin{frame}{First-Order Methods}
\framesubtitle{Stochastic Gradient Descent}
\small
\begin{algorithm}[H]
\caption{Stochastic Gradient Descent (SGD)}
\begin{algorithmic}[1]
\Require $\eta$: Learning rate
\Require $\mathcal{L}(\theta)$: Loss function
\Require $\theta_0$: Initial parameter
\State $t \gets 0$
\While{not converged}
    \State Sample $\xi_t$ \Comment{Randomly chosen data point}
    \State $g_t \gets \nabla_\theta \mathcal{L}(\theta_t; \xi_t)$ \Comment{Stochastic gradient}
    \State $\theta_{t+1} \gets \theta_t - \eta g_t$ \Comment{Parameter update}
    \State $t \gets t + 1$
\EndWhile
\State \Return $\theta_{t+1}$
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{frame}[fragile]{First-Order Methods}
\framesubtitle{Stochastic Gradient Descent}
\begin{minted}[fontsize=\small, bgcolor=gray!10, linenos]{python}
def stochastic_gradient_descent(grad_fn, theta0, eta, T, X, y):
    theta = theta0
    m = len(y)
    for t in range(T):
        i = np.random.randint(0, m)
        X_i = X[i:i+1]  
        y_i = y[i:i+1]
        g = m*grad_func(X_i, y_i, theta)
        theta = theta - eta * g
    return theta
\end{minted}
\end{frame}

\begin{frame}[fragile]{First-Order Methods}
\framesubtitle{Stochastic Gradient Descent}
\textbf{Problems:}
\begin{itemize}
    \item Gradient estimates are noisy $\Rightarrow$ cause oscillations near the optimum.
    \item High variance in updates $\Rightarrow$ slower and less stable convergence.
    \item May get trapped in local minima or saddle points.
    \item Highly sensitive to learning rate and not adaptive to feature scaling.
\end{itemize}
\end{frame}

\subsection{Momentum-Based Methods}
\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Heavy-Ball}
\begin{block}{Heavy Ball}
Update formula:
\[
\theta_{t+1}=\theta_t - \eta g(\theta_t;\xi_t) +\beta(\theta_t -\theta_{t-1})
\]
where
\begin{itemize}
    \item $0 \le \beta < 1$ : momentum coefficient (scales the previous velocity),
\end{itemize}
\end{block}
\begin{itemize}
    \item Introduce momentum $\Rightarrow$ Faster convergence.
    \item Introduce momentum $\Rightarrow$ Help escape local minima.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Heavy-Ball}
\textbf{Problems:}
\begin{itemize}
    \item How does $\beta$ affect the optimizer?  
    \item How does velocity affect the optimizer? 
\end{itemize}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Nestrov Accelerated Gradient}
\begin{block}{NAG - Nestrov Accelerated Gradient}
Update formula:
\[
\begin{cases}
\gamma_{k+1} = \theta_k - \eta g(\theta_k; \xi_k) \\
\theta_{k+1} = \gamma_{k+1} + \beta \big(\gamma_{k+1} - \gamma_k)
\end{cases}
\]
with initial velocity \(v_0 = 0\), where
\begin{itemize}
    \item $0 \le \beta < 1$ : momentum coefficient (scales the previous velocity),
\end{itemize}
\end{block}
\begin{itemize}
    \item \textbf{Faster convergence:} anticipates future position, correcting the direction earlier than standard momentum.
    \item \textbf{Reduced oscillations:} smoother trajectory near minima thanks to lookahead gradient.
    \item \textbf{Better stability:} less overshooting and improved performance on curved loss surfaces.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Nestrov Accelerated Gradient}
\textbf{Problems:}
\begin{itemize}
    \item Fine-tune hyperparameters. 
\end{itemize}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Unified Momentum}
\begin{block}{Unified Momentum}
Update formula:
\[
\text{UM}:\begin{cases}
\gamma_{t+1} = \theta_t - \eta g(\theta_t; \xi_t) \\
\gamma^{s}_{t+1} = \theta_t - s \eta g(\theta_t; \xi_t) \\
\theta_{t+1} = \gamma_{t+1} + \beta \big(\gamma^{s}_{t+1} - \gamma^{s}_t)
\end{cases}
\]
where
\begin{itemize}
    \item $0 \le \beta < 1$ : momentum coefficient,
    \item $s \ge 0$ : parameter controlling the variant (e.g., $s=0$ gives Heavy Ball, $s=1$ gives NAG, $s=1/\beta$ gives SGD),
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based Methods}
\framesubtitle{Unified Momentum}
\small
\begin{algorithm}[H]
\caption{Unified Momentum (UM)}
\begin{algorithmic}[1]
\Require $\eta$: Learning rate
\Require $\beta$: Momentum coefficient
\Require $s$: Scaling factor
\Require $\mathcal{L}(\theta)$: Loss function
\Require $\theta_0$: Initial parameter
\State $t \gets 0$
\While{not converged}
    \State $\gamma_{t+1} \gets \theta_t - \eta \, g(\theta_t; \xi_t)$
    \State $\gamma^{s}_{t+1} \gets \theta_t - s \eta \, g(\theta_t; \xi_t)$
    \State $\theta_{t+1} \gets \gamma_{t+1} + \beta (\gamma^{s}_{t+1} - \gamma^{s}_t)$
    \State $t \gets t + 1$
\EndWhile
\State \Return Final parameter $\theta_T$
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Momentum-Based}
\framesubtitle{Unified Momentum}
\begin{minted}[fontsize=\small, bgcolor=gray!10, linenos]{python}
def unified_momentum(grad_func, theta0, eta, beta, s, T, X, y):
    gamma = theta0.copy()
    gamma_s = theta0.copy()
    m = len(y)
    for t in range(T):
        i = np.random.randint(0, m)
        X_i = X[i:i+1]
        y_i = y[i:i+1]
        g = grad_func(X_i, y_i, theta0)
        gamma_next = theta0 - eta * g
        gamma_s_next = theta0 - s * eta * g
        theta0[:] = gamma_next + beta * (gamma_s_next - gamma_s)
        gamma[:] = gamma_next
        gamma_s[:] = gamma_s_next

\end{minted}
\end{frame}

\subsection{Adaptive Learning Rate Methods}
\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaGrad}
\begin{block}{AdaGrad}
Update formula:
\[
\text{AdaGrad}:\begin{cases}
\gamma_{t+1} = \gamma_{t} + \big(g(\theta_{t}; \xi_t)\big)^2 \\
\theta_{t} = \theta_{t} - \dfrac{\eta}{\sqrt{\gamma_{t}+\epsilon} }\, g(\theta_{t}; \xi_t)
\end{cases}
\]
where
\begin{itemize}
    \item $\epsilon$ helps control numerical stability
\end{itemize}

\end{block}
\begin{itemize}
    \item Automatically rescales the gradient along each parameter direction. 
\end{itemize}
\end{frame}
\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaGrad}
\small
\begin{algorithm}[H]
\caption{Adagrad}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, learning rate $\eta$
\Require Number of iterations $T$, stochastic gradient function $g(\theta;\xi)$
\Require small constant $\epsilon$
\State $\gamma_0 \gets 0$, $t \gets 1$, $\theta_0 \gets0$  \Comment{Initialize accumulated squared gradients}
\While{not converged}
    \State $g_t \gets g(\theta_t; \xi_t)$ \Comment{Compute stochastic gradient}
    \State $\gamma_{t} \gets \gamma_{t-1} + g_t \odot g_t$ \Comment{Element-wise square accumulation}
    \State $\theta_{t} \gets \theta_{t-1} - \eta \frac{g_t}{\sqrt{\gamma_t} + \epsilon}$ \Comment{Update parameters}
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaGrad}
\begin{minted}[fontsize=\small, bgcolor=gray!10, linenos]{python}
def adagrad(grad_func, theta0, eta, T, X, y, eps=1e-8):
    gamma = np.zeros_like(theta0) 
    m = len(y)
    
    for t in range(T):
        i = np.random.randint(0, m)
        X_i = X[i:i+1]
        y_i = y[i:i+1]
        g = m*grad_func(X_i, y_i, theta0)
        
        gamma[:] = gamma + g**2
        theta0[:] = theta0 - eta * g / (np.sqrt(gamma) + eps)
\end{minted}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaGrad}
\textbf{Problems:}
\begin{itemize}
    \item Accumulating the squared gradients over time.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{RMSProp}
\begin{block}{RMSProp}
Update formula:
\[
\text{RMSProp}:\begin{cases}
\gamma_{t+1} = \alpha\gamma_{t} + (1-\alpha)\big(g(\theta_{t}; \xi_t)\big)^2 \\
\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\gamma_{t}+\epsilon}} \odot\, g(\theta_{t}; \xi_t)
\end{cases}
\]
where
\begin{itemize}
    \item $\epsilon$ helps control numerical stability,
    \item $\alpha$ is the smooth constant
\end{itemize}
\begin{itemize}
    \item Exponentially decay the influence of past gradients.
\end{itemize}
\end{block}

\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{RMSProp}
\small
\begin{algorithm}[H]
\caption{RMSProp}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, learning rate $\eta$
\Require Number of iterations $T$, stochastic gradient function $g(\theta;\xi)$
\Require Smoothing constant $\alpha$, small constant $\epsilon$
\State $\gamma_0 \gets 0$, $t \gets 1$, $\theta_0 \gets0$  \Comment{Initialize accumulated squared gradients}
\While{not converged}
    \State $g_t \gets g(\theta_t; \xi_t)$ \Comment{Compute stochastic gradient}
    \State $\gamma_{t+1} \gets \alpha \gamma_{t} + (1-\alpha) g_t \odot g_t$ \Comment{Update running average}
    \State $\theta_{t+1} \gets \theta_t - \eta \frac{g_t}{\sqrt{\gamma_t} + \epsilon}$ \Comment{Update parameters}
    \State $t \gets t+1$
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{RMSProp}
\begin{minted}[fontsize=\small, bgcolor=gray!10, linenos]{python}
def rmsprop(grad_func, theta, eta, T, X, y, rho=0.9, eps=1e-8):
    gamma = np.zeros_like(theta)
    m_data = len(y)

    for t in range(T):
        i = np.random.randint(0, m_data)
        X_i = X[i:i+1]
        y_i = y[i:i+1]
        g_t = grad_func(X_i, y_i, theta)

        gamma[:] = rho * gamma + (1 - rho) * (g_t ** 2)
        theta[:] = theta - eta * g_t / (np.sqrt(gamma) + eps)

    return theta

\end{minted}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaDelta}
\begin{block}{AdaDelta}
Update formula:
\[
\text{AdaDelta}:\begin{cases}
\gamma_{t+1} = \rho\,\gamma_t + (1 - \rho)\,\big(g(\theta_t; \xi_t)\big)^2 \\
g'_{t+1}= \frac{\sqrt{\Delta \theta_t +\epsilon}} {\sqrt{\gamma_{t+1}+\epsilon}} \odot g(\theta_t, \xi_t)\\
\Delta \theta_{t+1} = \rho\Delta\theta_{t} +(1-\rho)g'^2_{t+1}\\
\theta_{t+1} = \theta_t - 
\eta g'_{t+1}
\end{cases}
\]
where
\begin{itemize}
    \item $\epsilon$ helps control numerical stability
    \item $\rho$ is the coefficient used for computing a running average of squared gradients
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaDelta}
\begin{algorithm}[H]
\caption{Adadelta}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, learning rate $\eta$
\Require Decay rate $\rho$, small constant $\varepsilon$
\State $\gamma_0 \gets 0$, $\Delta\theta_0 \gets 0$, $t \gets 0$ \Comment{Initialize variables}
\While{not converged}
    \State $g_t \gets g(\theta_t; \xi_t)$ \Comment{Compute stochastic gradient}
    \State $\gamma_{t+1} \gets \rho\,\gamma_t + (1 - \rho)\, (g_t \odot g_t)$ \Comment{Accumulate gradient energy}
    \State $g'_{t+1} \gets \frac{\sqrt{\Delta\theta_t + \varepsilon}}{\sqrt{\gamma_{t+1} + \varepsilon}} \odot g_t$ \Comment{Compute scaled gradient}
    \State $\Delta\theta_{t+1} \gets \rho\,\Delta\theta_t + (1 - \rho)\,(g'_{t+1} \odot g'_{t+1})$ \Comment{Update energy}
    \State $\theta_{t+1} \gets \theta_t - \eta\,g'_{t+1}$ \Comment{Update parameters}
    \State $t \gets t + 1$ \Comment{Increment iteration counter}
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{AdaDelta}
\begin{minted}[fontsize=\scriptsize, bgcolor=gray!10, linenos]{python}
def adadelta(grad_func, theta, T, X, y, rho=0.95, eps=1e-6):
    gamma = np.zeros_like(theta)       
    delta_theta = np.zeros_like(theta) 
    m = len(y)

    for t in range(T):
        i = np.random.randint(0, m)
        X_i = X[i:i+1]
        y_i = y[i:i+1]
        g_t = grad_func(X_i, y_i, theta)  

        gamma = rho * gamma + (1 - rho) * (g_t**2)  
        scaled_grad = (np.sqrt(delta_theta + eps) / np.sqrt(gamma + eps)) * g_t  
        theta = theta - scaled_grad 
        delta_theta = rho * delta_theta + (1 - rho) * (scaled_grad**2)  

    return theta
\end{minted}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{Adam}
\begin{block}{Adam}
Update formula:
\[
\text{Adam}:\begin{cases}
m_{t+1} = \beta_{1}m_{t} +g(\theta_{t};\xi_{t})    \\
\gamma_{t+1} = \beta_{2}\gamma_t + (1-\beta_2)(g(\theta_{t};\xi_{t}))^2 \\
\theta_{t+1} = \theta_t - \eta \frac{m_{t+1}}{1-\beta_1^t} \frac{1}{\sqrt{\frac{\gamma_t}{1-\beta_2^t}}+\epsilon}
\end{cases}
\]
where
\begin{itemize}

    \item $0 \le \beta_1,\beta_2 < 1$ : coefficients used for computing running averages of gradient and its square

\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{Adam}
\small
\begin{algorithm}[H]
\caption{Adam}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, learning rate $\eta$
\Require Exponential decay rates $\beta_1, \beta_2$, small constant $\epsilon$
\State $m_0 \gets 0$, $\gamma_0 \gets 0$, $t \gets 0$ \Comment{Initialize variables}
\While{not converged}
    \State $g_t \gets g(\theta_t; \xi_t)$ \Comment{Compute gradient}
    \State $m_{t+1} \gets \beta_1 m_t + (1-\beta_1) g_t$ \Comment{Update biased first moment}
    \State $\gamma_{t+1} \gets \beta_2 \gamma_t + (1-\beta_2) (g_t \odot g_t)$ \Comment{Update biased second moment}
    \State $\hat{m}_{t+1} \gets \frac{m_{t+1}}{1 - \beta_1^{t+1}}, \quad \hat{\gamma}_{t+1} \gets \frac{\gamma_{t+1}}{1 - \beta_2^{t+1}}$ \Comment{Bias-corrected estimates}
    \State $\theta_{t+1} \gets \theta_t - \eta \frac{\hat{m}_{t+1}}{\sqrt{\hat{\gamma}_{t+1}} + \epsilon}$ \Comment{Update parameters}
    \State $t \gets t + 1$ \Comment{Increment iteration counter}
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{frame}[fragile]{First-Order Methods - Adaptive Learning Rate Methods}
\framesubtitle{Adam}
\begin{minted}[fontsize=\scriptsize, bgcolor=gray!10, linenos]{python}
def adam(grad_func, theta, eta, T, X, y, beta1=0.9, beta2=0.999, eps=1e-8):
    m = np.zeros_like(theta)
    gamma = np.zeros_like(theta)
    m_data = len(y)

    for t in range(1, T+1):
        i = np.random.randint(0, m_data)
        X_i = X[i:i+1]
        y_i = y[i:i+1]
        g_t = grad_func(X_i, y_i, theta)

        m[:] = beta1 * m + (1 - beta1) * g_t
        gamma[:] = beta2 * gamma + (1 - beta2) * (g_t ** 2)

        m_hat = m / (1 - beta1**t)
        gamma_hat = gamma / (1 - beta2**t)

        theta[:] = theta - eta * m_hat / (np.sqrt(gamma_hat) + eps)

    return theta


\end{minted}
\end{frame}
% ======================
\section{Second-Order Methods} 
% ======================

\subsection{Newtonâ€™s Method}
\begin{frame}{Second-Order Methods - Newton's Method}
\framesubtitle{Newton's Method}
\begin{block}{Newton's Update Formula}
\[
\theta_{t+1} = \theta_t - H^{-1}(\theta_t; \mathcal{D}) \nabla \mathcal{L}(\theta_t; \mathcal{D})
\]
where:
\begin{itemize}
    \item \( H(\theta_t; \mathcal{D}) = \nabla^2 \mathcal{L}(\theta_t; \mathcal{D}) \), the Hessian matrix of the loss function \(\mathcal{L}(\theta_t; \mathcal{D})\) with respect to \(\theta_t\).
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[fragile]{Second-Order Methods - Newton's Method}
\framesubtitle{Newton's Method}

\small
\begin{algorithm}[H]
\caption{Newton's Method}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, maximum iterations $T$
\Require Objective function $J(\theta;\mathcal D)$, dataset $\mathcal D$
\State $t \gets 0$
\While{not converged}
    \State Compute gradient: $g_t \gets \nabla J(\theta_t; \mathcal D)$
    \State Compute Hessian: $H_t \gets \nabla^2 J(\theta_t; \mathcal D)$
    \State Update parameters: $\theta_{t+1} \gets \theta_t - H_t^{-1} g_t$
    \State $t \gets t + 1$
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}

\subsection{Quasi-Newton Methods}
\begin{frame}{Second-Order Methods - Quasi-Newton Methods}
\framesubtitle{BFGS}
\small
\begin{block}{Formula:}
\[
\theta_{t+1} = \theta_t - \eta_t\, B_t^{-1} \nabla \mathcal L(\theta_t; \mathcal D)
\]
\begin{itemize}
    \item $B_t$ is pseudo-Hessian
matrix Hessian $H_t = \nabla^2 \mathcal L(\theta_t; \mathcal D)$.

\end{itemize}
\end{block}

\end{frame}


\begin{frame}{Second-Order Methods - Quasi-Newton Methods}
\framesubtitle{BFGS}
\small
\begin{algorithm}[H]
\caption{BFGS Algorithm}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, 
\Require initial inverse Hessian approximation $B_0^{-1} = I$, learning rate $\eta_t$
\State $t \gets 0$
\While{not converged}
    \State Compute gradient: $g_t \gets \nabla J(\theta_t; \mathcal D)$
    \State Update parameters: $\theta_{t+1} \gets \theta_t - \eta_t B_t^{-1} g_t$
    \State Compute $s_t \gets \theta_{t+1} - \theta_t$
    \State Compute $y_t \gets g_{t+1} - g_t$
    \State 
    $B_{t+1}^{-1} \gets 
    \left(I - \frac{s_t y_t^\top}{y_t^\top s_t}\right)
    B_t^{-1}
    \left(I - \frac{y_t s_t^\top}{y_t^\top s_t}\right)
    + \frac{s_t s_t^\top}{y_t^\top s_t}$
    
    
    \State $t \gets t + 1$
\EndWhile
\State \Return $\theta_t$
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{frame}{Second-Order Methods - Quasi-Newton Methods}
\framesubtitle{LBFGS}
Sorry, this section is too hard for us to present. We may need Mrs Hang to teach us about this. 
\end{frame}

\section{Future Work}
\begin{frame}{Future Work}
	\begin{itemize}
		\item Explore more methods.
		\item Convergence Analysis
		\item Other methods of optimization.
	\end{itemize}
\end{frame}

\end{document}